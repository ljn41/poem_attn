<u>OpenNMT-py/attention.py</u>里面的GetAttenion类的get_attention是主要的接口 给get_attention函数输入上句和下句，可以返回一个numpy array的list，分别是下句对上半句每一个字的attention

比如输入 两个黄鹂鸣翠柳 一行白鹭上青天

返回：

> [(array([[0.2209934 , 0.12285779, 0.06889769, 0.17424507, 0.16967703,
>         0.11693747, 0.12639157]], dtype=float32),), (array([[0.03411091, 0.12179628, 0.01514272, 0.13434085, 0.45000535,
>         0.20891905, 0.03568489]], dtype=float32),), (array([[0.0879982 , 0.13243629, 0.05177321, 0.49322465, 0.15721054,
>         0.07303826, 0.00431889]], dtype=float32),), (array([[0.07978975, 0.31269836, 0.02188571, 0.28285247, 0.25331813,
>         0.04639728, 0.0030583 ]], dtype=float32),), (array([[0.03406094, 0.11718976, 0.06979868, 0.5394088 , 0.10414187,
>         0.13287485, 0.0025251 ]], dtype=float32),), (array([[7.4327397e-03, 2.3973224e-01, 5.0683230e-02, 5.6804198e-01,
>         4.4299535e-02, 8.9416414e-02, 3.9388111e-04]], dtype=float32),), (array([[5.3540367e-04, 7.7372782e-02, 2.8377198e-02, 7.1634412e-01,
>         6.2334608e-02, 1.1503512e-01, 8.2381439e-07]], dtype=float32),)]

返回了七个array，第一个array就是"一"对上一句每个字的attention，第二个array就是"行"对上一句每个字的attention，以此类推

要统计的是，后面一句诗每个字对上句话中attention最大的字对，比如对于"一"，attention分别是0.2209934 , 0.12285779, 0.06889769, 0.17424507, 0.16967703,0.11693747, 0.12639157，最大的是第一个，所有"一"和"二"这个tuple就加1。先不用考虑"一"和"二"谁attend谁，也就是不用考虑"一"和"二"的次序

如果用矩阵存的话太稀疏了，所以就用dict存吧，key是tuple，value是数量

因为作诗用的是二生一模型，所以除了第一句话到第二句话，其他都用前两句来生成下一句，所以统计的时候，除了第一句到第二句之外，其他时候输入都是前两句和当前句，比如下次输入的时候就应该是 两个黄鹂鸣翠柳,一行白鹭上青天 窗含西岭千秋雪 【注】因为我当时训练的时候两句话是用的英文逗号隔开的，所以这里也用英文的","吧      （实现的话应该不难但是可能略有些麻烦，辛苦啦～）

语料是那个<u>poem_corpus.txt</u>，每首之间有一个空行

总体上讲大概就是把句对整理好，然后不断调用<u>get_attention</u>就行

程序运行

```python
python3 attention.py -src data/src-test.txt -tgt data/tgt-test.txt -model checkpoints/checkpoints_512_512_2048_polished/demo_step_100000.pt
```

 